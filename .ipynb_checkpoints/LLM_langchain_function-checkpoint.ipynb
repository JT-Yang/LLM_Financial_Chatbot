{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f5e243fe",
   "metadata": {},
   "source": [
    "# Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1cae72a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import langchain_openai\n",
    "import tiktoken\n",
    "import chromadb\n",
    "import langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fe6a4cb7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rufen/anaconda3/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `run` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Nvidia's stock dipped 3% after the company unveiled its new generation of artificial intelligence chips called Blackwell. CEO Jensen Huang announced the new chips at Nvidia's developers conference in San Jose, California, touting them as even more powerful than the current generation of Hopper graphics processing units. The company also announced a new enterprise software product known as Nvidia Inference Microservice, which makes it easier to run older generations of Nvidia GPUs. Analysts at Bernstein, Wells Fargo, and Goldman Sachs maintained positive ratings and price targets for Nvidia stock, expressing optimism about the company's technology and innovation.\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "loader = WebBaseLoader(\"https://www.cnbc.com/2024/03/19/nvidia-stock-dips-after-company-unveils-latest-ai-chips.html\")\n",
    "docs = loader.load()\n",
    "\n",
    "llm = ChatOpenAI(temperature=0, model_name=\"gpt-3.5-turbo-1106\", openai_api_key=\"\")\n",
    "chain = load_summarize_chain(llm, chain_type=\"stuff\")\n",
    "\n",
    "chain.run(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "569dd9c7",
   "metadata": {},
   "source": [
    "<img src=\"summarization.png\" alt=\"Two common approaches to pass documents into LLMs context window\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a61e942",
   "metadata": {},
   "source": [
    "## Stuff\n",
    "\n",
    "When we use load_summarize_chain with chain_type=\"stuff\", we will use the StuffDocumentsChain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fd743069",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"On February 13, 2024, the stock market experienced a sell-off as hotter-than-expected inflation data for January raised concerns about the Federal Reserve's ability to cut rates multiple times this year. The Dow Jones Industrial Average lost 443 points, or 1.2%, while the S&P 500 and Nasdaq Composite fell 1.2% and 1.4% respectively. The consumer price index rose 0.3% in January, exceeding expectations, and core prices, which exclude volatile food and energy components, rose 0.4%. The spike in Treasury yields also contributed to the decline in tech stocks, including Microsoft and Amazon.\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains.combine_documents.stuff import StuffDocumentsChain\n",
    "from langchain.chains.llm import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "#define prompt\n",
    "prompt_template = \"\"\"write a concise summary of the following:\n",
    "\"{text}\"\n",
    "concise summry:\n",
    "\"\"\"\n",
    "prompt = PromptTemplate.from_template(prompt_template)\n",
    "\n",
    "#define LLm chain \n",
    "llm = ChatOpenAI(temperature=0, model_name=\"gpt-3.5-turbo-16k\", openai_api_key=\"\")\n",
    "llm_chain =  LLMChain(llm=llm, prompt = prompt)\n",
    "\n",
    "#define StuffDocumentsChain\n",
    "stuff_chain = StuffDocumentsChain(llm_chain=llm_chain, document_variable_name=\"text\")\n",
    "\n",
    "docs = loader.load()\n",
    "stuff_chain.run(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c377ebae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"- Stocks dropped on Tuesday after hotter-than-expected inflation data for January spiked Treasury yields and raised doubts about the Federal Reserve's ability to cut rates multiple times this year.\\n- The Dow Jones Industrial Average lost 443 points, or 1.2%, while the S&P 500 slid 1.2% and the Nasdaq Composite fell 1.4%.\\n- The consumer price index rose 0.3% in January from December, with economists expecting a 0.2% increase. Core prices, excluding food and energy, rose 0.4% month over month and 3.9% from a year ago.\\n- Tech shares, including Microsoft and Amazon, led the losses in trading Tuesday, with Microsoft sliding 1.4% and Amazon falling 1.4%.\\n- In corporate news, JetBlue Airways spiked 12% after activist investor Carl Icahn reported a nearly 10% stake in the airline, while Hasbro lost 6% after missing analyst expectations for the fourth quarter.\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#define prompt\n",
    "prompt = PromptTemplate.from_template(\n",
    "'''Please provide a bullet-point summary of the key points from the following document, \n",
    "   focusing on major events and findings. Ensure the summary is concise and under 100 words:\n",
    "\"{text}\"\n",
    "Summary:''')\n",
    "\n",
    "#define LLm chain \n",
    "#temperatures in the range of 0.1 to 0.7 might produce summaries that balance creativity and accuracy.\n",
    "llm = ChatOpenAI(temperature=0.2, model_name=\"gpt-3.5-turbo-16k\", openai_api_key=\"\")\n",
    "llm_chain =  LLMChain(llm=llm, prompt = prompt)\n",
    "\n",
    "#define StuffDocumentsChain\n",
    "stuff_chain = StuffDocumentsChain(llm_chain=llm_chain, document_variable_name=\"text\")\n",
    "\n",
    "docs = loader.load()\n",
    "stuff_chain.run(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c9e4859",
   "metadata": {},
   "source": [
    "# RAG  \n",
    "question-answering applications\n",
    "* A typical RAG had two main components: indexing and retrieval/generation\n",
    "* Indexing: a pipeline for ingesting data from a source and indexing it. This usually happens offline.\n",
    "* Retrieval and generation: the actual RAG chain, which takes the user query at run time and retrieves the relevant data from the index, then passes that to the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d62b3058",
   "metadata": {},
   "source": [
    "<img src=\"indexing.png\" alt=\"indexing\">\n",
    "<img src=\"retrieval_generation.png\" alt=\"RAG\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "100c0b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import langchain \n",
    "import langchain_community \n",
    "import langchain_openai \n",
    "import chromadb \n",
    "import bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8d0817b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "#os.environ[\"OPENAI_API_KEY\"] = getpass.getpass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d5de254d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "········\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = getpass.getpass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "479adcca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4\n",
    "from langchain import hub\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_core.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f4327310",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load, chunk and index the contents of the blog.\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            class_=(\"post-content\", \"post-title\", \"post-header\")  #can be customized \n",
    "        )\n",
    "    ),\n",
    ")\n",
    "docs = loader.load() #load\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)  #split\n",
    "splits = text_splitter.split_documents(docs)\n",
    "vectorstore = Chroma.from_documents(documents=splits, embedding=OpenAIEmbeddings(openai_api_key=\"\"))   #store\n",
    "\n",
    "\n",
    "# Retrieve and generate using the relevant snippets of the blog.\n",
    "retriever = vectorstore.as_retriever()\n",
    "#prompt = hub.pull(\"rlm/rag-prompt\")                          \n",
    "template = \"\"\"Use the following pieces of context to answer the question at the end.\n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "Use three sentences maximum and keep the answer as concise as possible.\n",
    "Always say \"thanks for asking!\" at the beginning of the answer.\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Helpful Answer:\"\"\"\n",
    "custom_rag_prompt = PromptTemplate.from_template(template)\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0,openai_api_key=\"\")\n",
    "\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | custom_rag_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "6c477700",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The approaches to Task Decomposition include using a Language Model with simple prompting, task-specific instructions, or human inputs. Thanks for asking!'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain.invoke(\"What are the approaches to Task Decomposition?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d6f20334",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The content of simple prompting is the use of a prompt to guide a language model in generating a response or completing a task. It involves providing specific instructions or cues to the model to elicit the desired output. Thanks for asking!'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain.invoke(\"what is the content of simple prompting?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "4732508f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleanup\n",
    "vectorstore.delete_collection()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f4e4db",
   "metadata": {},
   "source": [
    "### Step-by-step explaination and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d83f8437",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load docs\n",
    "from langchain.document_loaders import WebBaseLoader\n",
    "loader = WebBaseLoader(\"https://en.wikipedia.org/wiki/Jack_Ma\")\n",
    "data = loader.load()\n",
    "\n",
    "len(docs[0].page_content)\n",
    "\n",
    "# Split\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size = 500, chunk_overlap = 0)\n",
    "all_splits = text_splitter.split_documents(data)\n",
    "\n",
    "len(splits)\n",
    "len(splits[0].page_content)\n",
    "splits[10].metadata\n",
    "\n",
    "# Store splits\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "vectorstore = Chroma.from_documents(documents=all_splits, embedding=OpenAIEmbeddings())\n",
    "\n",
    "# RAG prompt\n",
    "from langchain import hub\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "# LLM\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "# RetrievalQA\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm,\n",
    "    retriever=vectorstore.as_retriever(),\n",
    "    chain_type_kwargs={\"prompt\": prompt}\n",
    ")\n",
    "\n",
    "question = \"What are the approaches to Task Decomposition?\"\n",
    "result = qa_chain({\"query\": question})\n",
    "result[\"result\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f017f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#retrieval \n",
    "retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 6})\n",
    "retrieved_docs = retriever.invoke(\"What are the approaches to Task Decomposition?\")\n",
    "len(retrieved_docs)\n",
    "print(retrieved_docs[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d08a9ed",
   "metadata": {},
   "source": [
    "# Chatbot\n",
    "* Chat models\n",
    "* Prompt Templates\n",
    "* Chat history\n",
    "* Retrievers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e71fcd98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import langchain_openai\n",
    "import langchain\n",
    "# import quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "5947dc37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "chat = ChatOpenAI(model=\"gpt-3.5-turbo-1106\", temperature=0.2, openai_api_key=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "988ca514",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a helpful assistant. Answer all questions to the best of your ability.\",\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = prompt | chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "09805b40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='hi!'), AIMessage(content='whats up?')]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.memory import ChatMessageHistory\n",
    "\n",
    "demo_ephemeral_chat_history = ChatMessageHistory()\n",
    "\n",
    "demo_ephemeral_chat_history.add_user_message(\"hi!\")\n",
    "\n",
    "demo_ephemeral_chat_history.add_ai_message(\"whats up?\")\n",
    "\n",
    "demo_ephemeral_chat_history.messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "cae66010",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='To build an interactive LLM (Language Model) application, you can use a programming language such as Python and a natural language processing library like TensorFlow or PyTorch. You would need to train a language model on a large dataset of text, and then use the model to generate responses based on user input. There are also pre-trained language models available, such as GPT-3, that you can use to build interactive applications. Additionally, you may want to consider using a framework like Flask or Django to create a web-based interface for your application.')"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demo_ephemeral_chat_history.add_user_message(\n",
    "    \"How can I build an interactive LLM application?\"\n",
    ")\n",
    "\n",
    "response = chain.invoke({\"messages\": demo_ephemeral_chat_history.messages})\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "ec77d91f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Certainly! There are many resources available for studying natural language processing and building interactive applications. Here are a few suggestions:\\n\\n1. Coursera and Udemy offer courses on natural language processing and machine learning that can provide a solid foundation for building language models and interactive applications.\\n\\n2. The TensorFlow and PyTorch websites provide extensive documentation, tutorials, and examples for building and training language models.\\n\\n3. \"Natural Language Processing in Action\" by Lane, Howard, and Hapke is a highly regarded book that covers the fundamentals of natural language processing and provides practical examples.\\n\\n4. GitHub is a great resource for finding open-source projects related to natural language processing and interactive applications. You can study the code and learn from the implementations of others.\\n\\n5. Online communities such as Stack Overflow, Reddit, and specialized forums like the TensorFlow and PyTorch communities are great places to ask questions and learn from others in the field.\\n\\nThese resources should provide you with a good starting point for studying natural language processing and building interactive applications. Good luck with your studies!')"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demo_ephemeral_chat_history.add_ai_message(response)\n",
    "\n",
    "demo_ephemeral_chat_history.add_user_message(\"Do you have any learnign resources for me to study?\")\n",
    "\n",
    "chain.invoke({\"messages\": demo_ephemeral_chat_history.messages})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3bdae0a",
   "metadata": {},
   "source": [
    "# Interactive chatbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "72053c93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting streamlit\n",
      "  Downloading streamlit-1.31.1-py2.py3-none-any.whl (8.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.4/8.4 MB\u001b[0m \u001b[31m34.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting altair<6,>=4.0\n",
      "  Downloading altair-5.2.0-py3-none-any.whl (996 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m996.9/996.9 kB\u001b[0m \u001b[31m34.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tornado<7,>=6.0.3 in /Users/rufen/anaconda3/lib/python3.10/site-packages (from streamlit) (6.2)\n",
      "Requirement already satisfied: packaging<24,>=16.8 in /Users/rufen/anaconda3/lib/python3.10/site-packages (from streamlit) (23.2)\n",
      "Requirement already satisfied: cachetools<6,>=4.0 in /Users/rufen/anaconda3/lib/python3.10/site-packages (from streamlit) (5.3.2)\n",
      "Collecting gitpython!=3.1.19,<4,>=3.0.7\n",
      "  Downloading GitPython-3.1.41-py3-none-any.whl (196 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.4/196.4 kB\u001b[0m \u001b[31m25.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: protobuf<5,>=3.20 in /Users/rufen/anaconda3/lib/python3.10/site-packages (from streamlit) (4.25.2)\n",
      "Requirement already satisfied: click<9,>=7.0 in /Users/rufen/anaconda3/lib/python3.10/site-packages (from streamlit) (8.0.4)\n",
      "Collecting blinker<2,>=1.0.0\n",
      "  Downloading blinker-1.7.0-py3-none-any.whl (13 kB)\n",
      "Collecting rich<14,>=10.14.0\n",
      "  Downloading rich-13.7.0-py3-none-any.whl (240 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m240.6/240.6 kB\u001b[0m \u001b[31m29.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tzlocal<6,>=1.1\n",
      "  Downloading tzlocal-5.2-py3-none-any.whl (17 kB)\n",
      "Requirement already satisfied: importlib-metadata<8,>=1.4 in /Users/rufen/anaconda3/lib/python3.10/site-packages (from streamlit) (6.11.0)\n",
      "Collecting validators<1,>=0.2\n",
      "  Downloading validators-0.22.0-py3-none-any.whl (26 kB)\n",
      "Requirement already satisfied: python-dateutil<3,>=2.7.3 in /Users/rufen/anaconda3/lib/python3.10/site-packages (from streamlit) (2.8.2)\n",
      "Requirement already satisfied: tenacity<9,>=8.1.0 in /Users/rufen/anaconda3/lib/python3.10/site-packages (from streamlit) (8.2.3)\n",
      "Requirement already satisfied: pillow<11,>=7.1.0 in /Users/rufen/anaconda3/lib/python3.10/site-packages (from streamlit) (9.4.0)\n",
      "Requirement already satisfied: numpy<2,>=1.19.3 in /Users/rufen/anaconda3/lib/python3.10/site-packages (from streamlit) (1.23.5)\n",
      "Requirement already satisfied: requests<3,>=2.27 in /Users/rufen/anaconda3/lib/python3.10/site-packages (from streamlit) (2.29.0)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.3.0 in /Users/rufen/anaconda3/lib/python3.10/site-packages (from streamlit) (4.9.0)\n",
      "Collecting pydeck<1,>=0.8.0b4\n",
      "  Downloading pydeck-0.8.1b0-py2.py3-none-any.whl (4.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.8/4.8 MB\u001b[0m \u001b[31m39.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pandas<3,>=1.3.0 in /Users/rufen/anaconda3/lib/python3.10/site-packages (from streamlit) (1.5.3)\n",
      "Collecting pyarrow>=7.0\n",
      "  Downloading pyarrow-15.0.0-cp310-cp310-macosx_11_0_arm64.whl (24.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.2/24.2 MB\u001b[0m \u001b[31m36.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: toml<2,>=0.10.1 in /Users/rufen/anaconda3/lib/python3.10/site-packages (from streamlit) (0.10.2)\n",
      "Requirement already satisfied: jsonschema>=3.0 in /Users/rufen/anaconda3/lib/python3.10/site-packages (from altair<6,>=4.0->streamlit) (4.17.3)\n",
      "Requirement already satisfied: toolz in /Users/rufen/anaconda3/lib/python3.10/site-packages (from altair<6,>=4.0->streamlit) (0.12.0)\n",
      "Requirement already satisfied: jinja2 in /Users/rufen/anaconda3/lib/python3.10/site-packages (from altair<6,>=4.0->streamlit) (3.1.2)\n",
      "Collecting gitdb<5,>=4.0.1\n",
      "  Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: zipp>=0.5 in /Users/rufen/anaconda3/lib/python3.10/site-packages (from importlib-metadata<8,>=1.4->streamlit) (3.17.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/rufen/anaconda3/lib/python3.10/site-packages (from pandas<3,>=1.3.0->streamlit) (2022.7)\n",
      "Requirement already satisfied: six>=1.5 in /Users/rufen/anaconda3/lib/python3.10/site-packages (from python-dateutil<3,>=2.7.3->streamlit) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/rufen/anaconda3/lib/python3.10/site-packages (from requests<3,>=2.27->streamlit) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/rufen/anaconda3/lib/python3.10/site-packages (from requests<3,>=2.27->streamlit) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/rufen/anaconda3/lib/python3.10/site-packages (from requests<3,>=2.27->streamlit) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/rufen/anaconda3/lib/python3.10/site-packages (from requests<3,>=2.27->streamlit) (2023.5.7)\n",
      "Collecting markdown-it-py>=2.2.0\n",
      "  Downloading markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.5/87.5 kB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/rufen/anaconda3/lib/python3.10/site-packages (from rich<14,>=10.14.0->streamlit) (2.15.1)\n",
      "Collecting smmap<6,>=3.0.1\n",
      "  Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/rufen/anaconda3/lib/python3.10/site-packages (from jinja2->altair<6,>=4.0->streamlit) (2.1.1)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /Users/rufen/anaconda3/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.18.0)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /Users/rufen/anaconda3/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (22.1.0)\n",
      "Collecting mdurl~=0.1\n",
      "  Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Installing collected packages: validators, tzlocal, smmap, pyarrow, mdurl, blinker, pydeck, markdown-it-py, gitdb, rich, gitpython, altair, streamlit\n",
      "Successfully installed altair-5.2.0 blinker-1.7.0 gitdb-4.0.11 gitpython-3.1.41 markdown-it-py-3.0.0 mdurl-0.1.2 pyarrow-15.0.0 pydeck-0.8.1b0 rich-13.7.0 smmap-5.0.1 streamlit-1.31.1 tzlocal-5.2 validators-0.22.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install streamlit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "42a1bf43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit\n",
    "import openai\n",
    "import langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "a59a27e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-14 11:47:45.390 \n",
      "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
      "  command:\n",
      "\n",
      "    streamlit run /Users/rufen/anaconda3/lib/python3.10/site-packages/ipykernel_launcher.py [ARGUMENTS]\n"
     ]
    }
   ],
   "source": [
    "# a simple question-answering chatbox\n",
    "import streamlit as st\n",
    "#from langchain_community.llms import OpenAI\n",
    "\n",
    "st.title('Auto-Summarization App')\n",
    "\n",
    "openai_api_key = st.sidebar.text_input('OpenAI API Key', type='password')\n",
    "\n",
    "def generate_response(input_text):\n",
    "    llm = OpenAI(temperature=0.7, openai_api_key=openai_api_key)\n",
    "    st.info(llm(input_text))\n",
    "\n",
    "with st.form('my_form'):\n",
    "    text = st.text_area('Enter text:', 'What are the three key pieces of advice for learning how to code?')\n",
    "    submitted = st.form_submit_button('Submit')\n",
    "    if not openai_api_key.startswith('sk-'):\n",
    "        st.warning('Please enter your OpenAI API key!', icon='⚠')\n",
    "    if submitted and openai_api_key.startswith('sk-'):\n",
    "        generate_response(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "422e63ec",
   "metadata": {},
   "source": [
    "# SQL \n",
    "* sql agent\n",
    "* interact with csv data file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51544695",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sql agent \n",
    "from langchain_community.agent_toolkits import create_sql_agent\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    "agent_executor = create_sql_agent(llm, db=db, agent_type=\"openai-tools\", verbose=True)\n",
    "\n",
    "# then use natural language to ask your question "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "976aa6f7",
   "metadata": {},
   "source": [
    "# CSV\n",
    "\n",
    "*  use pandas, this approach is not fit for production use cases unless you have extensive safeguards in place"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f054548",
   "metadata": {},
   "outputs": [],
   "source": [
    "# can be deleted?\n",
    "ai_msg = llm.invoke(\n",
    "    \"I have a pandas DataFrame 'df' with columns 'Age' and 'Fare'. Write code to compute the correlation between the two columns. Return Markdown for a Python code snippet and nothing else.\"\n",
    ")\n",
    "print(ai_msg.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d746e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_experimental.tools import PythonAstREPLTool\n",
    "\n",
    "df = pd.read_csv(\"titanic.csv\")\n",
    "tool = PythonAstREPLTool(locals={\"df\": df})\n",
    "tool.invoke(\"df['Fare'].mean()\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "406a75c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_with_tools = llm.bind_tools([tool], tool_choice=tool.name)\n",
    "llm_with_tools.invoke(\n",
    "    \"I have a dataframe 'df' and want to know the correlation between the 'Age' and 'Fare' columns\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab02104",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers.openai_tools import JsonOutputKeyToolsParser\n",
    "\n",
    "parser = JsonOutputKeyToolsParser(tool.name, return_single=True)\n",
    "(llm_with_tools | parser).invoke(\n",
    "    \"I have a dataframe 'df' and want to know the correlation between the 'Age' and 'Fare' columns\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb3f1a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "from langchain_core.messages import ToolMessage\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import MessagesPlaceholder\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "system = f\"\"\"You have access to a pandas dataframe `df`. \\\n",
    "Here is the output of `df.head().to_markdown()`:\n",
    "\n",
    "```\n",
    "{df.head().to_markdown()}\n",
    "```\n",
    "\n",
    "Given a user question, write the Python code to answer it. \\\n",
    "Don't assume you have access to any libraries other than built-in Python ones and pandas.\n",
    "Respond directly to the question once you have enough information to answer it.\"\"\"\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            system,\n",
    "        ),\n",
    "        (\"human\", \"{question}\"),\n",
    "        # This MessagesPlaceholder allows us to optionally append an arbitrary number of messages\n",
    "        # at the end of the prompt using the 'chat_history' arg.\n",
    "        MessagesPlaceholder(\"chat_history\", optional=True),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "def _get_chat_history(x: dict) -> list:\n",
    "    \"\"\"Parse the chain output up to this point into a list of chat history messages to insert in the prompt.\"\"\"\n",
    "    ai_msg = x[\"ai_msg\"]\n",
    "    tool_call_id = x[\"ai_msg\"].additional_kwargs[\"tool_calls\"][0][\"id\"]\n",
    "    tool_msg = ToolMessage(tool_call_id=tool_call_id, content=str(x[\"tool_output\"]))\n",
    "    return [ai_msg, tool_msg]\n",
    "\n",
    "\n",
    "chain = (\n",
    "    RunnablePassthrough.assign(ai_msg=prompt | llm_with_tools)\n",
    "    .assign(tool_output=itemgetter(\"ai_msg\") | parser | tool)\n",
    "    .assign(chat_history=_get_chat_history)\n",
    "    .assign(response=prompt | llm | StrOutputParser())\n",
    "    .pick([\"tool_output\", \"response\"])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "703262b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain.invoke({\"question\": \"What's the correlation between age and fare\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96875d17",
   "metadata": {},
   "source": [
    "# Tagging (label document)\n",
    "use case:\n",
    "* sentiment\n",
    "* language\n",
    "* style (formal, informal etc.)\n",
    "* covered topics\n",
    "* political tendency"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdee0b18",
   "metadata": {},
   "source": [
    "<img src=\"tagging.png\" alt=\"tag\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6887e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple example\n",
    "# Schema\n",
    "schema = {\n",
    "    \"properties\": {\n",
    "        \"sentiment\": {\"type\": \"string\"},\n",
    "        \"aggressiveness\": {\"type\": \"integer\"},\n",
    "        \"language\": {\"type\": \"string\"},\n",
    "    }\n",
    "}\n",
    "\n",
    "# LLM\n",
    "llm = ChatOpenAI(temperature=0, model=\"gpt-3.5-turbo-0613\")\n",
    "chain = create_tagging_chain(schema, llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfbe59aa",
   "metadata": {},
   "source": [
    "# Extraction\n",
    "* synthetic data generation\n",
    "* how to extract data using langchain\n",
    "* pydantic_schema = (a class)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62cbe11b",
   "metadata": {},
   "source": [
    "<img src=\"extraction.png\" alt=\"ext\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ba1bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_extraction_chain\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Schema\n",
    "schema = {\n",
    "    \"properties\": {\n",
    "        \"name\": {\"type\": \"string\"},\n",
    "        \"height\": {\"type\": \"integer\"},\n",
    "        \"hair_color\": {\"type\": \"string\"},\n",
    "    },\n",
    "    \"required\": [\"name\", \"height\"],\n",
    "}\n",
    "\n",
    "# Input\n",
    "inp = \"\"\"Alex is 5 feet tall. Claudia is 1 feet taller Alex and jumps higher than him. Claudia is a brunette and Alex is blonde.\"\"\"\n",
    "\n",
    "# Run chain\n",
    "llm = ChatOpenAI(temperature=0, model=\"gpt-3.5-turbo\")\n",
    "chain = create_extraction_chain(schema, llm)\n",
    "chain.run(inp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "386cecbc",
   "metadata": {},
   "source": [
    "# Web Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49833c74",
   "metadata": {},
   "source": [
    "<img src=\"web.png\" alt=\"web scraping\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a02fd77a",
   "metadata": {},
   "source": [
    "# Interacte with APIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1adc94e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.openai_functions.openapi import get_openapi_chain\n",
    "\n",
    "chain = get_openapi_chain(\n",
    "    \"https://www.klarna.com/us/shopping/public/openai/v0/api-docs/\"\n",
    ")\n",
    "chain(\"What are some options for a men's large blue button down shirt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd09840",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import APIChain\n",
    "from langchain.chains.api import open_meteo_docs\n",
    "from langchain_openai import OpenAI\n",
    "\n",
    "llm = OpenAI(temperature=0, openai_api_key=\"\")\n",
    "chain = APIChain.from_llm_and_api_docs(\n",
    "    llm,\n",
    "    open_meteo_docs.OPEN_METEO_DOCS,\n",
    "    verbose=True,\n",
    "    limit_to_domains=[\"https://api.open-meteo.com/\"],\n",
    ")\n",
    "chain.run(\n",
    "    \"What is the weather like right now in Munich, Germany in degrees Fahrenheit?\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
